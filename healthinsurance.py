# -*- coding: utf-8 -*-
"""HealthInsurance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12N2X3bFRpNkGoPXAmjv4vQNSVGApy7-F

# EDA
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegressionCV
from sklearn.neural_network import MLPClassifier
from imblearn.over_sampling import SMOTE

from google.colab import drive
drive.mount('/content/drive')

# Exploratory Data Analysis (EDA)
dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/HealthInsurance/healthInsurance.csv')
pd.set_option('display.max_columns', None)

print(dataset.info())
print(dataset.head())

# Check NaN value
check_nan = dataset.isnull().values.any()
count_nan = dataset.isnull().sum().sum()
print('\n\nNaN values: ', check_nan)
print('NaN count: ', count_nan)

# Remove duplicated samples
print('\n\nDataset shape: ', dataset.shape)
duplicate_rows = dataset[dataset.duplicated()]
print('#Duplicated rows: ', duplicate_rows.shape)

# Dummyfying non numerical features
columns_to_dummy = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']
dataset = pd.get_dummies(dataset, columns=columns_to_dummy)
print(dataset.head(10))

# Remove useless binary features
columns_to_drop = ['Gender_Female', 'Vehicle_Damage_No']
dataset.drop(columns_to_drop, axis=1, inplace=True)

# Rename complementary features of deleted ones
dataset.rename(columns={'Gender_Male': 'Gender', 'Vehicle_Damage_Yes': 'Vehicle_Damage'}, inplace=True)
print(dataset.info())

# Check for unbalanced features
columns_to_check = ['Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel',
                    'Gender', 'Vehicle_Damage', 'Vehicle_Age_1-2 Year',
                    'Vehicle_Age_< 1 Year', 'Vehicle_Age_> 2 Years', 'Response']
plt.figure(figsize=(20, 12))
for idx, value in enumerate(columns_to_check):
    print("\n", value, "count: ", dataset[value].value_counts())
    print(value, "Normalized count: ", dataset[value].value_counts(normalize=True))
    plt.subplot(3, 4, idx + 1)
    dataset[value].value_counts(normalize=True).plot.pie()
plt.show()

# Dropping feature Driving_License
# 1 = 0.998 && 0 = 0.002
dataset.drop('Driving_License', axis=1, inplace=True)

# Check collinearity between features
corrMatrix = dataset.corr()
plt.figure()
sn.heatmap(corrMatrix, annot=True)
plt.show()

# Checking correlation between Annual_Premium and Age
plt.figure()
plt.scatter(dataset['Age'], dataset['Annual_Premium'])
plt.show()

# Boxplots
columns_to_boxplot = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']
plt.figure(figsize=(12, 6))
dataset.boxplot(columns_to_boxplot)
plt.show()

# Checking for HLP for Annual_Premium feature
plt.figure()
dataset.boxplot('Annual_Premium')
plt.show()

# Removing 1% of the highest values
dataset = dataset[(dataset.Annual_Premium < dataset.Annual_Premium.quantile(.99))]
plt.figure()
dataset.boxplot('Annual_Premium')
plt.show()

# Features normalization
features_to_normalize = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']
scaler = preprocessing.MinMaxScaler()
dataset[features_to_normalize] = scaler.fit_transform(dataset[features_to_normalize])
print(dataset.head())

"""# Classification"""

# Model Evaluation Functions
def model_scores(y_test, y_pred, y_pred_proba, show_plot):
    print('Confusion matrix :\n', confusion_matrix(y_test, y_pred))
    print('Accuracy :', accuracy_score(y_test, y_pred))
    print('AUC score :', roc_auc_score(y_test, y_pred_proba))
    print('F1-score :', f1_score(y_test, y_pred))
    print('Precision score :', precision_score(y_test, y_pred))
    print('Recall score :', recall_score(y_test, y_pred))
    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba)
    plt.plot(fpr_test, tpr_test, label="ROC Curve")
    plt.xlabel('FPR')
    plt.ylabel('TPR')
    plt.legend()
    if show_plot:
        plt.show()

def model_evaluation(model, X_train, y_train, X_test, y_test, trained=False):
    if not trained:
        model.fit(X_train, y_train)
    y_pred = model.predict(X_train)
    y_pred_proba = model.predict_proba(X_train)[:, 1]
    print('Train dataset :')
    model_scores(y_train, y_pred, y_pred_proba, show_plot=False)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    print('\nTest dataset :')
    model_scores(y_test, y_pred, y_pred_proba, show_plot=True)

# Classification
TEST_SIZE = .2
dataset.drop('id', axis=1, inplace=True, errors='ignore') # Dropping 'id' column if exists, ignoring errors if it doesn't
print(dataset.head())
# dataset.drop('id', axis=1, inplace=True)  # This line is causing the error - remove it

# Split dataset into train and test set
y = dataset['Response'].values
X = dataset.drop('Response', axis=1).values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=1, stratify=y)

# Check if label are balanced
print("Train/Test shape:", X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print("Test set % of labels == 1:", y_test.sum() / y.sum() * 100)
# Label are balanced

print("\n##### LOGISTIC REGRESSION #####")
# Logistic Regression
log_reg = LogisticRegressionCV()
clf = GridSearchCV(log_reg, {'Cs': [1, 3, 5, 10], 'cv': [3, 5]}, n_jobs=-1, cv=5)
clf.fit(X_train, y_train)
print('Best parameters found:\n', clf.best_params_)
model_evaluation(clf, X_train, y_train, X_test, y_test, trained=True)

# Dataset oversampling
smote = SMOTE(sampling_strategy='minority')
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

#  Check if label are balanced
print("Oversampled train shape:", X_train_sm.shape, y_train_sm.shape)
print("Original train shape:", X_train.shape, y_train.shape)
print("#1 in oversampled train:", y_train_sm.sum())
print("#1 in original train:", y_train.sum())
# Labels are equally distributed

# Logistic Regression with oversampled train
print("\n##### LOGISTIC REGRESSION (oversampled data) #####")
clf = GridSearchCV(log_reg, {'Cs': [1, 3, 5, 10], 'cv': [3, 5]}, n_jobs=-1, cv=5)
clf.fit(X_train_sm, y_train_sm)
print('Best parameters found:\n', clf.best_params_)
model_evaluation(clf, X_train_sm, y_train_sm, X_test, y_test, trained=True)

# Neural network parameters
parameter_space = {
    'hidden_layer_sizes': [(8, 5), (7,), (10, 3), (30,), (50, 50, 50), (50, 100, 50), (100,)],
    'alpha': [0.0001, 0.1, 1, 3, 5],
    'learning_rate': ['constant', 'adaptive'],
}

# Neural Network
print("\n##### NEURAL NETWORKS #####")
# mpl = MLPClassifier(max_iter=500)
# Searching best parameters
# clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)
# clf.fit(X_train, y_train)
# Best parameter set
# print('Best parameters found:\n', clf.best_params_)
# {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}
# model_evaluation(clf, X_train, y_train, X_test, y_test, trained=True)
mlp = MLPClassifier(max_iter=500, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate='adaptive')
model_evaluation(mlp, X_train, y_train, X_test, y_test, trained=False)

# Neural network with oversampled train
print("\n##### NEURAL NETWORKS (oversampled data) #####")
# mpl = MLPClassifier(max_iter=500)
# Searching best parameters
# clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)
# clf.fit(X_train_sm, y_train_sm)
# Best parameter set
# print('Best parameters found:\n', clf.best_params_)
# {'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive'}
# model_evaluation(clf, X_train_sm, y_train_sm, X_test, y_test, trained=True)
mlp = MLPClassifier(max_iter=500, alpha=0.0001, hidden_layer_sizes=(50, 100, 50), learning_rate='adaptive')
model_evaluation(mlp, X_train_sm, y_train_sm, X_test, y_test, trained=False)

plt.show()